{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale Content-Based Image Retrieval (CBIR)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 The image retrieval problem**  \n",
    "<img src=\"images/CBIR.png\" height=\"10\" width=\"400\" align=\"right\">\n",
    "\n",
    "\n",
    "Content-based image retrieval (CBIR) is the process of searching for images in a large database, given a query of search. Technically, there are three key issue in CBIR:  \n",
    "  1. Image representation\n",
    "  2. Database organisation\n",
    "  3. Image distance measurement\n",
    "\n",
    "\n",
    "\n",
    "We can further specify the definition of CBIR above.\n",
    "\n",
    "> CIBR makes use of the ***representation*** of visual content to identify relevant images in a database.\n",
    "\n",
    "\n",
    "\n",
    "<!-- Most traditional and common methods of image retrieval system utilize some method of adding metadata such as captioning, keywords, or descriptions to the images so that retrieval can be performed over the annotation words. The image retrieval classified mainly in three types:\n",
    "\n",
    "       a. Text Based Image Retrieval  \n",
    "       b. Content Based Image Retrieval  \n",
    "       c. Sketch Based Image Retrieval  \n",
    "       d. Query based Image Retrieval  \n",
    "       e. Semantic Based Image Retrieval  \n",
    "       f. Annotation based image retrieval  \n",
    " -->\n",
    "<u>In this lesson we will focus on a **Query-based Image Retrieval** problem, which uses an example image as query.</u>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we would coceptually implement a CBIR model.  \n",
    "We will divide the CBIR into different functionalities and we will conquer each functionality later in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBIR:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 CBIR pipeline**  \n",
    "We can distinguish two main stages in a CBIR framework: an **offline** stage, and an **online** stage [ref].  \n",
    "\n",
    "<div style=\"img {align: left}\">\n",
    "<img src=\"images/pipeline.png\">\n",
    "<em>Add image caption with reference</em>\n",
    "</div>\n",
    "\n",
    "The objective of the **offline** stage is to use the image stack to build an indexed database. We tend to concentrate the computational effort in this stage of creation, to ease, instead, the online stage.\n",
    "It is concerned with two main operations:\n",
    "- Create a representation of the images\n",
    "- Efficiently index the images \n",
    "\n",
    "\n",
    "\n",
    "Given a *query* image, the objective of the **online** stage is to score part (or all) of the images in the database, and return the ones with the higest scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBIR:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        return\n",
    "\n",
    "    def encode(self, image):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def build_index(self, dataset):\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def score(self, image):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Offline stage: building the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin:auto; float:right; margin-left: 50px; width: 40%\">\n",
    "<img src=\"images/features_extraction_only.jpeg\">\n",
    "<em>Add image caption with reference</em>\n",
    "</div>\n",
    "\n",
    "**1.1 Features extraction**  \n",
    "\n",
    "In CBIR an image is transformed to some kind of feature space. The motivation is to achieve an implicit alignment so as to eliminate the impact of background and potential transformations or changes while keeping the intrinsic visual content distinguishable.\n",
    "\n",
    "\n",
    "On the other hand, we will implement the \"Bag of visual words\" model [ref]. It borrows the cocept of \"bag of words\" from the natural language processing field, and implements its visual correspondent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: auto; float: left; margin-right: 50px; width: 19%\">\n",
    "<img src=\"images/bow.png\">\n",
    "<em>Add image caption with reference</em>\n",
    "</div>\n",
    "\n",
    "**1.2 Image representation**  \n",
    "\n",
    "On the other hand, we will implement the \"Bag of visual words\" model [ref]. It borrows the \"bag of words\" concept from the natural language processing field, and implements a visual surrogate.\n",
    "\n",
    "We will implement features extraction and bag of words creation using the [Scale Invariant Feature Transform (SIFT)](https://link.springer.com/content/pdf/10.1023/B:VISI.0000029664.99615.94.pdf) method. \n",
    "\n",
    "\n",
    "SIFT is an image descriptor for image-based matching and recognition developed by David Lowe (1999, 2004). This descriptor as well as related image descriptors are used for a large number of purposes in computer vision related to point matching between different views of a 3-D scene and view-based object recognition. The SIFT descriptor is invariant to translations, rotations and scaling transformations in the image domain and robust to moderate perspective transformations and illumination variations. Experimentally, the SIFT descriptor has been proven to be very useful in practice for image matching and object recognition under real-world conditions. [ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. Features indexing**  \n",
    "Vocabulary tree - https://ieeexplore.ieee.org/document/1641018  \n",
    "[We add some descriptive text here, ideally from literature, and diagrams to help visualise the concept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Online stage: get the _n_ most similar instances from a query image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Image Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Reindexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. End-to-end image retrieval\n",
    "Here we bring together all the concepts we have illustrated above to build our Large-Scale CBIR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [Optional] Features extraction with Deep Convolutional Neural Networks\n",
    "The excercises below this point are not mandatory. They provide a wider picture on how to build an efficient image representation. We will illustrate two techniques that will require elements of Deep Learning:\n",
    "- 4.1 Using pretrained deep artificial neural networks to build a representation of the image\n",
    "- 4.2 Fine tune a pretrained model on our database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Using a pre-trained network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either:\n",
    "- Illustrate http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf and use denoising autoencoders,\n",
    "- Or go more basic and use a DCNN (https://arxiv.org/abs/1404.1777)\n",
    "\n",
    "Regardless, I think that the best option is to use a pre-trained network - ResNet50?.  \n",
    "\n",
    "We can add also another step, if we have the time and people have the will, in wich we fine tune the network (only last layer?) on our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Fine-tuning a pre-trained network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# sbearbank\n",
       "\n",
       "### Getting started\n",
       "\n",
       "Download the data\n",
       "```\n",
       "python download.py\n",
       "```\n",
       "\n",
       "### Literature\n",
       "---\n",
       "\n",
       "#### Datasets:\n",
       "- [Hamming embedding and weak geometricconsistency for large scale image search (INRIA Holydays)](http://lear.inrialpes.fr/people/jegou/data.php#holidays) - [download](https://lear.inrialpes.fr/pubs/2008/JDS08/jegou_hewgc08.pdf)\n",
       "- [Large-scale Landmark Retrieval/Recognition under a Noisy and Diverse Dataset](https://arxiv.org/abs/1906.04087)\n",
       "- [INSTRE: a New Benchmark for Instance-Level Object Retrieval and Recognition](https://dl.acm.org/doi/pdf/10.1145/2700292)\n",
       "\n",
       "#### Database indexing:\n",
       "- [PQk-means: Billion-scale Clustering forProduct-quantized Codes](https://arxiv.org/pdf/1709.03708.pdf)\n",
       "- [Scalable Recognition with a Vocabulary Tree](https://ieeexplore.ieee.org/document/1641018)\n",
       "- [Object retrieval with large vocabularies and fast spatial matching](https://ieeexplore.ieee.org/document/4270197)\n",
       "\n",
       "#### Features extraction:\n",
       "- [Neural Codes for Image Retrieval](https://arxiv.org/pdf/1404.1777.pdf)\n",
       "- [Scale-Invariant Feature Transform](https://pdfs.semanticscholar.org/0129/3b985b17154fbb178cd1f944ce3cc4fc9266.pdf)\n",
       "- [Object Recognition from Local Scale-Invariant Features](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790410)\n",
       "- [Distinctive Image Features from Scale-Invariant Keypoints](https://link.springer.com/content/pdf/10.1023/B:VISI.0000029664.99615.94.pdf)\n",
       "- [Speeded Up Ro- bust Feature (SURF)](https://www.vision.ee.ethz.ch/~surf/eccv06.pdf)\n",
       "- [Using very deep autoencoders for content-based image retrieval](http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf)\n",
       "- [Video Google: A Text Retrieval Approach to Object Matching in Videos](http://www.robots.ox.ac.uk/~vgg/publications/papers/sivic03.pdf)\n",
       "\n",
       "#### End-to-end\n",
       "- [Large Scale Online Learning of Image Similarity Through Ranking (Triplet loss)](http://www.jmlr.org/papers/volume11/chechik10a/chechik10a.pdf)\n",
       "- [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737)\n",
       "\n",
       "#### Surveys\n",
       "- [A survey on Image Retrieval Methods](http://cogprints.org/9815/1/Survey%20on%20Image%20Retrieval%20Methods.pdf)\n",
       "- [Recent Advance in Content-based ImageRetrieval: A Literature Survey](https://arxiv.org/pdf/1706.06064.pdf)\n",
       "\n",
       "#### Non scientific\n",
       "- https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(open(\"README.md\", \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
