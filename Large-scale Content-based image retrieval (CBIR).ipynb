{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale Content-Based Image Retrieval\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 The image retrieval problem\n",
    "<img src=\"images/CBIR.png\" height=\"10\" width=\"400\" align=\"right\">\n",
    "\n",
    "\n",
    "Content-based image retrieval (CBIR) is the process of searching for images in a large database, given a query of search. Technically, there are three key issue in CBIR:  \n",
    "  1. Image representation\n",
    "  2. Database organisation\n",
    "  3. Image distance measurement\n",
    "\n",
    "\n",
    "\n",
    "We can further specify the definition of CBIR above.\n",
    "\n",
    "> CIBR makes use of the ***representation*** of visual content to identify relevant images in a database.\n",
    "\n",
    "\n",
    "<u>In this lesson we will focus on a **Query-based Image Retrieval** problem, which uses an example image as query.</u>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the end of this session, we expect to be able to have a function as below.  \n",
    "As we go through the notebook we will learn all the ingredients to implement such function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(database, query):\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 CBIR pipeline\n",
    "We can distinguish two main stages in a CBIR framework: an **offline** stage, and an **online** stage [ref].  \n",
    "\n",
    "<div style=\"img {align: left}\">\n",
    "<img src=\"images/pipeline.png\">\n",
    "<em>Add image caption with reference</em>\n",
    "</div>\n",
    "\n",
    "The objective of the **offline** stage is to use the image stack to build an indexed database. We tend to concentrate the computational effort in this stage of creation, to ease, instead, the online stage.\n",
    "It is concerned with two main operations:\n",
    "- Create a representation of the images\n",
    "- Efficiently index the images \n",
    "\n",
    "\n",
    "\n",
    "Given a *query* image, the objective of the **online** stage is to score part (or all) of the images in the database, and return the ones with the higest scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we would implement a CBIR model.  \n",
    "We will divide the CBIR into different functionalities and conquer each functionality as we go through the notebook. In the last part of the session, we will put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBIR:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.database = None  # we haven't built an indexed database yet\n",
    "        return\n",
    "    \n",
    "    #-- TASK 1\n",
    "    def extract_features(self, image):\n",
    "        # Implement me using SIFT, please\n",
    "        raise NotImplemented\n",
    "    \n",
    "    #-- TASK 2\n",
    "    def encode(self, image):\n",
    "        # Implement me using extract_features and BOW, please\n",
    "        raise NotImplemented\n",
    "\n",
    "    #-- TASK 3\n",
    "    def build_index(self, dataset):\n",
    "        # Implement me using vocabulary tree, please\n",
    "        raise NotImplemented\n",
    "    \n",
    "    #-- TASK 4\n",
    "    def score(self, image):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    #-- TASK 5\n",
    "    def retrieve(self, query):\n",
    "        # this is the function we have described above\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Offline stage: building the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Features extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin:auto; float:right; margin-left: 50px; width: 45%\">\n",
    "<img src=\"images/features_extraction_only.jpeg\">\n",
    "<em>Add image caption with reference Add image caption with reference Add image caption with reference Add image caption with reference Add image caption with reference Add image caption with reference Add image caption with reference</em>\n",
    "</div>\n",
    "\n",
    "\n",
    "<p style=\"color: #a00; font-weight: 700\">>> TASK 1</p>\n",
    "\n",
    "> In this section we are going to learn how to implement the function:\n",
    "```python\n",
    "def extract_features(self, image):\n",
    "    raise features\n",
    "```\n",
    "\n",
    "We implement features extraction using the [Scale Invariant Feature Transform (SIFT)](https://link.springer.com/content/pdf/10.1023/B:VISI.0000029664.99615.94.pdf) method. SIFT is an image descriptor for image-based matching and recognition developed by David Lowe (1999, 2004). This descriptor as well as related image descriptors are used for a large number of purposes in computer vision related to point matching between different views of a 3-D scene and view-based object recognition. The SIFT descriptor is invariant to translations, rotations and scaling transformations in the image domain and robust to moderate perspective transformations and illumination variations.[ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(self, image):\n",
    "    # Implement me using SIFT, please\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: auto; float: left; margin-right: 50px; width: 19%\">\n",
    "<img src=\"images/bow.png\">\n",
    "<em>Add image caption with reference</em>\n",
    "</div>\n",
    "\n",
    "**2.2 Image representation**\n",
    "<div style=\"margin-left: 230px\">\n",
    "<br>\n",
    "    \n",
    "    \n",
    "<p style=\"color: #a00; font-weight: 700\">>> TASK 2</p>\n",
    "    \n",
    "> In this section we are going to learn how to implement the function:\n",
    "```python\n",
    "def encode(self, image):\n",
    "    return embedding\n",
    "```\n",
    "</div>\n",
    "\n",
    "In CBIR an image is transformed to some kind of feature space. The motivation is to achieve an implicit alignment so as to eliminate the impact of background and potential transformations or changes while keeping the intrinsic visual content distinguishable. [ref]\n",
    "\n",
    "\n",
    "Once we have obtained the features of an image, ee will implement the \"Bag of visual words\" model [ref]. It borrows the cocept of \"bag of words\" from the natural language processing field, and implements its visual correspondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(self, image):\n",
    "    # Implement me using extract_features and BOW, please\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Features indexing\n",
    "\n",
    "<div style=\"margin:auto; float:right; margin-left: 50px; width: 30%\">\n",
    "<img src=\"images/vocabulary_tree.png\">\n",
    "<em>Add image caption with reference</em>\n",
    "</div>\n",
    "<br>\n",
    "    \n",
    "<p style=\"color: #a00; font-weight: 700\">>> TASK 3</p>\n",
    "\n",
    "> In this section we are going to learn how to implement the function:\n",
    "```python\n",
    "def build_index(dataset):\n",
    "    return indexed_database\n",
    "```\n",
    "\n",
    "In this section we provide a structure to the set of image representations collected in **1.2**.  \n",
    "We implement the [**Vocabulary tree structure**](https://ieeexplore.ieee.org/document/1641018) illustrated by Nister [ref], which uses inverted indices and hierarchical k-means to build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(dataset):\n",
    "    # Implement me using vocabulary tree, please\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Online stage: get the _n_ most similar instances from a query image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Image Scoring**  \n",
    "<!-- <div style=\"margin-left: 230px\"> -->\n",
    "<br>\n",
    "\n",
    "\n",
    "<p style=\"color: #a00; font-weight: 700\">>> TASK 4</p>\n",
    "\n",
    "> In this section we are going to learn how to implement the function:\n",
    "```python\n",
    "def score(database, image):\n",
    "    return score\n",
    "```\n",
    "<!-- </div> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Reindexing**\n",
    "\n",
    "Maybe this is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. End-to-end image retrieval\n",
    "Here we bring together all the concepts we have illustrated above to build our Large-Scale CBIR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [Optional] Features extraction with Deep Convolutional Neural Networks\n",
    "The excercises below this point are not mandatory. They provide a wider picture on how to build an efficient image representation. We will illustrate two techniques that will require elements of Deep Learning:\n",
    "- 4.1 Using pretrained deep artificial neural networks to build a representation of the image\n",
    "- 4.2 Fine tune a pretrained model on our database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Using a pre-trained network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either:\n",
    "- Illustrate http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf and use denoising autoencoders,\n",
    "- Or go more basic and use a DCNN (https://arxiv.org/abs/1404.1777)\n",
    "\n",
    "Regardless, I think that the best option is to use a pre-trained network - ResNet50?.  \n",
    "\n",
    "We can add also another step, if we have the time and people have the will, in wich we fine tune the network (only last layer?) on our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add some code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Fine-tuning a pre-trained network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# sbearbank\n",
       "\n",
       "### Literature\n",
       "\n",
       "#### Datasets:\n",
       "- [Hamming embedding and weak geometricconsistency for large scale image search (INRIA Holydays)](http://lear.inrialpes.fr/people/jegou/data.php#holidays) - [download](https://lear.inrialpes.fr/pubs/2008/JDS08/jegou_hewgc08.pdf)\n",
       "- [Large-scale Landmark Retrieval/Recognition under a Noisy and Diverse Dataset](https://arxiv.org/abs/1906.04087)\n",
       "- [INSTRE: a New Benchmark for Instance-Level Object Retrieval and Recognition](https://dl.acm.org/doi/pdf/10.1145/2700292)\n",
       "\n",
       "#### Features search:\n",
       "- [PQk-means: Billion-scale Clustering forProduct-quantized Codes](https://arxiv.org/pdf/1709.03708.pdf)\n",
       "- [Scalable Recognition with a Vocabulary Tree](https://ieeexplore.ieee.org/document/1641018)\n",
       "- [Object retrieval with large vocabularies and fast spatial matching](https://ieeexplore.ieee.org/document/4270197)\n",
       "\n",
       "#### Features extraction:\n",
       "- [Neural Codes for Image Retrieval](https://arxiv.org/pdf/1404.1777.pdf)\n",
       "- [Scale-Invariant Feature Transform (SIFT)](https://pdfs.semanticscholar.org/0129/3b985b17154fbb178cd1f944ce3cc4fc9266.pdf)\n",
       "- [Speeded Up Ro- bust Feature (SURF)](https://www.vision.ee.ethz.ch/~surf/eccv06.pdf)\n",
       "- [Using very deep autoencoders for content-based image retrieval](http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf)\n",
       "- [Video Google: A Text Retrieval Approach to Object Matching in Videos](http://www.robots.ox.ac.uk/~vgg/publications/papers/sivic03.pdf)\n",
       "\n",
       "#### End-to-end\n",
       "- [Large Scale Online Learning of Image Similarity Through Ranking (Triplet loss)](http://www.jmlr.org/papers/volume11/chechik10a/chechik10a.pdf)\n",
       "- [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737)\n",
       "\n",
       "#### Surveys\n",
       "- [A survey on Image Retrieval Methods](http://cogprints.org/9815/1/Survey%20on%20Image%20Retrieval%20Methods.pdf)\n",
       "- [Recent Advance in Content-based ImageRetrieval: A Literature Survey](https://arxiv.org/pdf/1706.06064.pdf)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(open(\"../README.md\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
